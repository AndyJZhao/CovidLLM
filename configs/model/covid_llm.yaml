# @package _global_
defaults:
  - /prompt/prompt_tree_meta_data
model:
  name: CovidLLM

# @ GraphText Settings
#graph_construct: spd1.spd2
subgraph_size: 5
#  title: 1stHopOnly
nb_padding: false
nb_order: true
node_dropout: 0.5

target: t1


# @ Demo for In-Context-Learning
use_demo: true
demo:
  #  select_method: first # Fixed seed examples for every sample
  select_method: max_degree # Fixed seed examples for every sample
  template: '{prompt_tree_info}The answer is {label}.'
  #  select: class-prototype # Select center of each class cluster
  #  select: BM25 # Use BM25 for dynamic retrieval
  #  select: BM25 # Randomly select seed examples
  keep_label_description: False
  n_separators: 2 # Number of separators between examples
  n_samples: ${data.n_labels} # Number of demonstrations


# @ Agent settings
agent_name: DeepSpeedAgent
local_rank: 0
save_path: ${out_dir}checkpoints/

# @ Text Settings
add_pre_instruction: true
pre_instruction_template: short
#remove_quotation: true
remove_quotation: true

# @ LLM
# LoRA
lora:
  r: 16
  alpha: ${.r}
  dropout: 0.1
  target_modules: [ q_proj, v_proj, k_proj, o_proj ]
#  modules_to_save: [ embed_tokens, lm_head ]

# @ EVALUATE
add_loop_inference: false
metrics: [ 'acc' ]
eval_embeds_path: null
eval_sets: [ 'train', 'val' , 'test' ]
#eval_sets: [ 'train']
choice_readout_pos: 0
min_eval_step: 10
log_path: null
exp_name: null
max_tgt_len: 2048 # the maximum sequence length to be generated
max_gen_len: 5 # the maximum sequence length to be generated
stage: 2
eval_freq: 30
save_freq: 30000
max_epochs: 9999
total_steps: 700 #Number of train steps
#use_lora: true
use_lora: false
use_embeds: true
frozen_encoder: false
frozen_llm: true
#frozen_encoder: true
#conv_template: covid_llm_v1
conv_template: no_conv
save_file: ${out_dir}${alias}.csv
mode: train
log_freq: 500
ds_config_path: configs/dsconfig/openllama_peft_stage_${stage}.json
use_flash_attn: false
# @ Text2Text fields: Upper

dropout: 0.5
eval_choice_only: ${add_class_token}
alias: ${llm.name}-${data.alias}-${target}

#
add_class_token: true
add_label_name_output: true
add_info_token: true
add_pad_token: true
#
eval_metric: val_acc

# ! Deepspeed related
use_deepspeed: false # For debug only
eq_batch_size: 4
inf_batch_size: ${oc.select:model._meta_data.inf_bsz,12}
max_bsz_per_gpu: ${oc.select:llm._meta_data.max_bsz_per_gpu,12}
bsz_per_gpu: ${get_bsz_per_gpu:${eq_batch_size}, ${max_bsz_per_gpu}}
grad_acc_steps: ${get_grad_acc_steps:${eq_batch_size}, ${max_bsz_per_gpu}}

# ! Float
use_fp16: true
use_bf16: true
optimizer_type: AdamW

# ! Optimizer
warmup_rate: 0.1
lr: 5e-5

ds: # Deepspeed config
  train_batch_size: ${eq_batch_size}
  train_micro_batch_size_per_gpu: ${bsz_per_gpu}
  gradient_accumulation_steps: ${grad_acc_steps} # ! To be overwritten
  steps_per_print: 2000
  gradient_clipping: 1.0
  zero_optimization:
    stage: 2 # ??? # Original 2
    offload_optimizer:
      device: cpu
    contiguous_gradients: true
    allgather_bucket_size: 500000000
    allgather_partitions: true

  fp16:
    enabled: ${use_fp16}
    opt_level: O2
    min_loss_scale: 1

  bf16:
    enable: ${use_bf16}

  optimizer:
    type: ${optimizer_type}
    params:
      lr: ${lr}
      betas: [ 0.9, 0.95 ]
      eps: 1e-8
      weight_decay: 0.001

  scheduler:
    type: WarmupDecayLR
    params:
      warmup_min_lr: 0
      warmup_max_lr: ${lr}
      warmup_num_steps: ${round_mult:${total_steps}, ${warmup_rate}}
      total_num_steps: ${total_steps}

  activation_checkpointing:
    partition_activations: true
    cpu_checkpointing: true
    contiguous_memory_optimization: false
    number_checkpoints: null
    synchronize_checkpoint_boundary: false
    profile: false